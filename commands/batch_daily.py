#!/usr/bin/env python3
"""
Optimized batch processing for daily operations.

This command is designed for efficient daily pipeline runs where you want to:
1. Add multiple individuals without immediate processing
2. Batch process all new individuals together  
3. Minimize redundant company reprocessing

Usage:
    python -m commands.batch_daily --add "person1:Company A" --add "person2:Company B"
    python -m commands.batch_daily --process-pending
    python -m commands.batch_daily --add "person1:Company A" --process-immediately
"""

import argparse
import logging
import time
from Database import Database
from individuals import update_spending_by_individuals
from process_individual_contributions import process_individual_contributions
from company_spending import update_spending_by_company
from process_company_contributions import process_company_contributions


def add_individuals_batch(individuals_data, process_immediately=False):
    """
    Add multiple individuals efficiently for daily operations.
    
    Args:
        individuals_data: List of tuples (individual_id, individual_data)
        process_immediately: Whether to process all at once after adding
    
    Returns:
        dict: Summary of operations
    """
    db = Database()
    db.get_constants()
    
    # Add all individuals to constants first
    current_individuals = db.individuals.copy()
    added_individuals = []
    
    for individual_id, individual_data in individuals_data:
        if individual_id in current_individuals:
            logging.warning(f"Individual '{individual_id}' already exists, skipping")
            continue
            
        # Ensure id field is set
        if "id" not in individual_data:
            individual_data["id"] = individual_id
            
        current_individuals[individual_id] = individual_data
        added_individuals.append((individual_id, individual_data))\n    \n    if not added_individuals:\n        return {\"status\": \"no_new_individuals\", \"added_count\": 0}\n    \n    # Update constants in Firestore\n    db.client.collection(\"constants\").document(\"individuals\").set(current_individuals)\n    db.individuals = current_individuals\n    \n    logging.info(f\"Added {len(added_individuals)} individuals to constants\")\n    \n    result = {\n        \"added_individuals\": [ind_id for ind_id, _ in added_individuals],\n        \"added_count\": len(added_individuals),\n        \"status\": \"added\",\n        \"processed\": False\n    }\n    \n    if process_immediately:\n        result.update(process_pending_individuals(db, [ind_id for ind_id, _ in added_individuals]))\n    \n    return result\n\n\ndef process_pending_individuals(db=None, specific_individuals=None):\n    \"\"\"\n    Process individuals that have been added but not yet processed.\n    Optimized for daily batch operations.\n    \"\"\"\n    if db is None:\n        db = Database()\n        db.get_constants()\n    \n    # Determine which individuals to process\n    if specific_individuals:\n        individuals_to_process = specific_individuals\n    else:\n        # Find individuals without processed data\n        individuals_to_process = []\n        for ind_id in db.individuals.keys():\n            existing_data = db.client.collection(\"individuals\").document(ind_id).get()\n            if not existing_data.exists or not existing_data.to_dict().get(\"contributions\"):\n                individuals_to_process.append(ind_id)\n    \n    if not individuals_to_process:\n        return {\n            \"status\": \"no_pending_individuals\",\n            \"processed_count\": 0,\n            \"companies_updated\": 0\n        }\n    \n    logging.info(f\"Processing {len(individuals_to_process)} individuals: {individuals_to_process}\")\n    start_time = time.time()\n    \n    # Step 1: Fetch contribution data for these individuals\n    temp_individuals = {ind_id: db.individuals[ind_id] for ind_id in individuals_to_process}\n    original_individuals = db.individuals\n    db.individuals = temp_individuals\n    \n    try:\n        new_contributions = update_spending_by_individuals(db)\n        logging.info(f\"Fetched {len(new_contributions)} total contributions\")\n    finally:\n        db.individuals = original_individuals\n    \n    # Step 2: Process individual contributions\n    new_recipients = process_individual_contributions(db)\n    logging.info(f\"Found {len(new_recipients)} new recipients\")\n    \n    # Step 3: Update company data efficiently\n    affected_companies = get_affected_companies(db, individuals_to_process)\n    companies_updated = 0\n    \n    if affected_companies:\n        logging.info(f\"Updating {len(affected_companies)} affected companies: {list(affected_companies)}\")\n        \n        # Update company data selectively\n        companies_updated = update_companies_selective(db, affected_companies)\n    \n    elapsed_time = time.time() - start_time\n    \n    return {\n        \"status\": \"processed\",\n        \"processed\": True,\n        \"processed_individuals\": individuals_to_process,\n        \"processed_count\": len(individuals_to_process),\n        \"new_contributions\": len(new_contributions),\n        \"new_recipients\": len(new_recipients),\n        \"affected_companies\": list(affected_companies),\n        \"companies_updated\": companies_updated,\n        \"elapsed_time_seconds\": round(elapsed_time, 2),\n        \"optimization\": \"batch_selective_processing\"\n    }\n\n\ndef get_affected_companies(db, individual_ids):\n    \"\"\"\n    Get the set of companies that need updating based on the individuals processed.\n    \"\"\"\n    affected_companies = set()\n    \n    for ind_id in individual_ids:\n        if ind_id in db.individuals:\n            individual = db.individuals[ind_id]\n            if \"company\" in individual:\n                affected_companies.update(individual[\"company\"])\n    \n    return affected_companies\n\n\ndef update_companies_selective(db, company_names):\n    \"\"\"\n    Update only the specific companies that are affected by the new individuals.\n    \"\"\"\n    # Find company IDs that match the names\n    company_ids_to_update = []\n    for company_id, company in db.companies.items():\n        if company[\"name\"] in company_names:\n            company_ids_to_update.append(company_id)\n    \n    if not company_ids_to_update:\n        logging.warning(f\"No company IDs found for names: {company_names}\")\n        return 0\n    \n    # Update company spending for these companies\n    original_companies = db.companies.copy()\n    temp_companies = {cid: db.companies[cid] for cid in company_ids_to_update}\n    db.companies = temp_companies\n    \n    try:\n        update_spending_by_company(db)\n    finally:\n        db.companies = original_companies\n    \n    # Process company contributions for these specific companies\n    from commands.add_individual import update_company_contributions_selective\n    update_company_contributions_selective(db, company_ids_to_update)\n    \n    return len(company_ids_to_update)\n\n\ndef parse_individual_spec(spec):\n    \"\"\"\n    Parse individual specification: \"id:Company Name\" or \"id:Company1,Company2\" or just \"id\"\n    \"\"\"\n    parts = spec.split(\":\", 1)\n    individual_id = parts[0].strip()\n    \n    individual_data = {\n        \"id\": individual_id,\n        \"name\": individual_id.replace(\"-\", \" \").title()\n    }\n    \n    if len(parts) > 1 and parts[1].strip():\n        companies = [c.strip() for c in parts[1].split(\",\") if c.strip()]\n        if companies:\n            individual_data[\"company\"] = companies\n    \n    return individual_id, individual_data\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Batch processing for daily operations\")\n    parser.add_argument(\"--add\", action=\"append\", help=\"Add individual (format: 'id:Company Name' or 'id:Company1,Company2')\")\n    parser.add_argument(\"--process-pending\", action=\"store_true\", help=\"Process individuals that haven't been processed yet\")\n    parser.add_argument(\"--process-immediately\", action=\"store_true\", help=\"Process individuals immediately after adding\")\n    parser.add_argument(\"--verbose\", \"-v\", action=\"store_true\", help=\"Verbose logging\")\n    \n    args = parser.parse_args()\n    \n    # Set up logging\n    level = logging.DEBUG if args.verbose else logging.INFO\n    logging.basicConfig(level=level, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n    \n    try:\n        total_result = {\"operations\": []}\n        \n        # Add individuals if specified\n        if args.add:\n            individuals_data = []\n            for spec in args.add:\n                individual_id, individual_data = parse_individual_spec(spec)\n                individuals_data.append((individual_id, individual_data))\n            \n            print(f\"ğŸ“ Adding {len(individuals_data)} individuals...\")\n            add_result = add_individuals_batch(individuals_data, args.process_immediately)\n            total_result[\"operations\"].append({\"type\": \"add\", \"result\": add_result})\n            \n            if add_result[\"added_count\"] > 0:\n                print(f\"âœ… Added {add_result['added_count']} individuals: {', '.join(add_result['added_individuals'])}\")\n                \n                if add_result.get(\"processed\"):\n                    print(f\"âš¡ Processed immediately (optimization: {add_result.get('optimization', 'unknown')})\")\n                    print(f\"ğŸ“Š Total contributions: {add_result.get('new_contributions', 0)}\")\n                    print(f\"ğŸ¢ Companies updated: {add_result.get('companies_updated', 0)}\")\n                    print(f\"â±ï¸  Elapsed time: {add_result.get('elapsed_time_seconds', 0)}s\")\n            else:\n                print(\"â„¹ï¸  No new individuals were added (may already exist)\")\n        \n        # Process pending individuals if specified\n        if args.process_pending and not args.process_immediately:\n            print(\"âš™ï¸  Processing pending individuals...\")\n            process_result = process_pending_individuals()\n            total_result[\"operations\"].append({\"type\": \"process\", \"result\": process_result})\n            \n            if process_result[\"processed_count\"] > 0:\n                print(f\"âœ… Processed {process_result['processed_count']} individuals\")\n                print(f\"ğŸ“Š New contributions: {process_result['new_contributions']}\")\n                print(f\"ğŸ¢ Companies updated: {process_result['companies_updated']}\")\n                print(f\"â±ï¸  Elapsed time: {process_result['elapsed_time_seconds']}s\")\n                print(f\"ğŸš€ Optimization: {process_result['optimization']}\")\n            else:\n                print(\"â„¹ï¸  No pending individuals found to process\")\n        \n        # Show usage tips for daily operations\n        if not args.add and not args.process_pending:\n            print(\"ğŸ’¡ Daily Operations Tips:\")\n            print(\"   Add without processing: --add 'person-id:Company Name'\")\n            print(\"   Process all pending: --process-pending\")\n            print(\"   Add and process immediately: --add 'person-id:Company' --process-immediately\")\n        \n        return 0\n        \n    except Exception as e:\n        print(f\"âŒ Error in batch processing: {e}\")\n        if args.verbose:\n            import traceback\n            traceback.print_exc()\n        return 1\n\n\nif __name__ == \"__main__\":\n    exit(main())